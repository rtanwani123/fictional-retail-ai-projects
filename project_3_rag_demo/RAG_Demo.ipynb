{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usdu8MXBk_sP"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PROJECT OVERVIEW — Fictional Retail Co. RAG Demo\n",
        "# ============================================================\n",
        "# This project builds a simple Retrieval-Augmented Generation (RAG) system\n",
        "# using only free and open-source tools.\n",
        "#\n",
        "# RAG means we first \"retrieve\" the most relevant text from documents,\n",
        "# and then \"generate\" an answer using a language model.\n",
        "#\n",
        "# Example:\n",
        "# Fictional Retail Co. has internal documents.\n",
        "# We want to ask questions like:\n",
        "#   - What is the return policy?\n",
        "#   - How can customers reach support?\n",
        "# and get accurate answers from those documents.\n",
        "#\n",
        "# ============================================================\n",
        "# SIMPLE EXPLANATION OF EACH STEP\n",
        "# ============================================================\n",
        "#\n",
        "# Step 1 — Install the Tools\n",
        "# We start by installing all the computer “tools” (Python libraries) we’ll need.\n",
        "# These include:\n",
        "# - Transformers and Sentence-Transformers → for understanding and generating text.\n",
        "# - FAISS → a super-fast search engine for finding similar text.\n",
        "# - PyPDF2 → to read PDF files.\n",
        "# - Gradio → to build a simple chatbot interface.\n",
        "#\n",
        "# Think of this step as putting all your ingredients on the kitchen counter before cooking.\n",
        "#\n",
        "# Step 2 — Import the Tools\n",
        "# Now that everything is installed, we tell Python we want to use them in our code using \"import\".\n",
        "# We also set up where our data (PDFs) is stored — in a folder called /content/fictional_retail_docs.\n",
        "#\n",
        "# Step 3 — Read & Split the Documents\n",
        "# We open each PDF and extract the text.\n",
        "# Then we split the big text into smaller chunks (like cutting a large paragraph into smaller paragraphs).\n",
        "#\n",
        "# Why? Because the model can’t read super long text at once — it’s like giving it a few sentences at a time to focus better.\n",
        "# We use:\n",
        "#   - chunk_size = 1000 characters (each piece)\n",
        "#   - overlap = 200 (to give some overlap so it remembers context)\n",
        "#\n",
        "# Step 4 — Turn Text into Numbers (Embeddings) & Build a Search Index\n",
        "# Computers don’t understand words directly — they understand numbers.\n",
        "# So, we use a model called all-MiniLM-L12-v2 to convert each text chunk into a set of numbers (called embeddings).\n",
        "# Then we store all these embeddings in FAISS, a kind of searchable “brain” that can quickly find which text chunks\n",
        "# are most similar to a question.\n",
        "#\n",
        "# Step 5 — Load the Answer Generator (FLAN-T5)\n",
        "# Now we load a text generation model — flan-t5-base — from Google.\n",
        "# This model can read a question and some context, and then write a short, clear answer.\n",
        "#\n",
        "# Step 6 — Ask Questions Safely (RAG Query Function)\n",
        "# Here we connect everything together.\n",
        "# When we ask a question:\n",
        "#   1. The system searches FAISS to find the most relevant chunks (top-k).\n",
        "#   2. It limits how much text is passed to the model (to avoid errors).\n",
        "#   3. It gives this text to FLAN-T5 and asks it to generate an answer.\n",
        "#\n",
        "# Step 7 — Test the System\n",
        "# We test it with a few sample questions — one from each file.\n",
        "# This helps confirm that the model is reading the right chunks and giving sensible answers.\n",
        "# It also prints which PDF file the answer came from, so you know the source.\n",
        "#\n",
        "# Step 8 — Make It Interactive with Gradio\n",
        "# Finally, we build a simple chat interface where anyone can type a question and get an answer.\n",
        "# This makes your project more user-friendly — like a mini AI chatbot for Fictional Retail Co.\n",
        "#\n",
        "# ============================================================\n",
        "# SYSTEM FLOW\n",
        "# ============================================================\n",
        "# 1. Load and split documents\n",
        "# 2. Turn text into embeddings\n",
        "# 3. Store embeddings in FAISS\n",
        "# 4. When user asks a question:\n",
        "#       → Find most similar text pieces\n",
        "#       → Give them to the model\n",
        "#       → Generate and return the answer\n",
        "# 5. Optional: Ask more questions via the Gradio interface\n",
        "#\n",
        "# ============================================================\n",
        "# TOOLS USED (All Free)\n",
        "# ============================================================\n",
        "# - SentenceTransformer: all-MiniLM-L12-v2 (for embeddings)\n",
        "# - Hugging Face Model: google/flan-t5-base (for generating answers)\n",
        "# - FAISS: for searching similar text\n",
        "# - PyPDF2: for reading PDFs\n",
        "# - Gradio: for the chatbot interface\n",
        "#\n",
        "# Everything in this project is open-source and 100% free.\n",
        "# No paid API keys or cloud accounts are needed.\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 1: Install Libraries\n",
        "# =========================\n",
        "!pip install --quiet sentence-transformers transformers faiss-cpu PyPDF2 gradio\n",
        "\n",
        "# =========================\n",
        "# Step 2: Import Libraries\n",
        "# =========================\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Folder containing your documents\n",
        "folder_path = \"/content/fictional_retail_docs\"\n",
        "\n",
        "# =========================\n",
        "# Step 3: Read and Split PDFs\n",
        "# =========================\n",
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"Split long text into smaller overlapping chunks for better processing.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "file_names = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        pdf = PdfReader(os.path.join(folder_path, filename))\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text() or \"\"\n",
        "            text += page_text.strip() + \" \"\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "        file_names.extend([filename] * len(chunks))\n",
        "\n",
        "print(f\"Loaded {len(all_chunks)} text chunks from {len(os.listdir(folder_path))} PDFs.\")\n",
        "\n",
        "# =========================\n",
        "# Step 4: Create Embeddings and Build FAISS Index\n",
        "# =========================\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
        "embeddings = embed_model.encode(all_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"FAISS index created and populated with document chunks.\")\n",
        "\n",
        "# =========================\n",
        "# Step 5: Load the Answer Generation Model\n",
        "# =========================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "gen_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# =========================\n",
        "# Step 6: Define the RAG Query Function\n",
        "# =========================\n",
        "def rag_query(query, k=2, max_new_tokens=200):\n",
        "    \"\"\"Retrieve top-k relevant chunks and generate an answer.\"\"\"\n",
        "    query_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "    retrieved_chunks = [all_chunks[i] for i in indices[0]]\n",
        "    sources = [file_names[i] for i in indices[0]]\n",
        "\n",
        "    # Limit total context size to avoid long input issues\n",
        "    context = \" \".join(retrieved_chunks)\n",
        "    if len(context) > 3500:\n",
        "        context = context[:3500]\n",
        "\n",
        "    # Build the prompt for FLAN-T5\n",
        "    prompt = f\"Answer the question based only on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
        "    response = gen_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0]['generated_text']\n",
        "\n",
        "    return {\"answer\": response, \"sources\": sources}\n",
        "\n",
        "# =========================\n",
        "# Step 7: Test the System\n",
        "# =========================\n",
        "sample_questions = {\n",
        "    \"Returns\": \"What is the return policy?\",\n",
        "    \"Warranty\": \"How long is the warranty period?\",\n",
        "    \"Customer Support\": \"How can customers contact support?\",\n",
        "    \"Loyalty\": \"How does the loyalty program work?\"\n",
        "}\n",
        "\n",
        "print(\"\\nTesting RAG system...\\n\")\n",
        "for topic, question in sample_questions.items():\n",
        "    result = rag_query(question)\n",
        "    print(f\"Question ({topic}): {question}\")\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Source Documents: {result['sources']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# =========================\n",
        "# Step 8: Gradio Interface\n",
        "# =========================\n",
        "def ask_rag(query):\n",
        "    result = rag_query(query)\n",
        "    answer = result[\"answer\"]\n",
        "    source = \", \".join(set(result[\"sources\"]))\n",
        "    return f\"Answer: {answer}\\n\\nSource Documents: {source}\"\n",
        "\n",
        "demo = gr.Interface(fn=ask_rag, inputs=\"text\", outputs=\"text\", title=\"Fictional Retail Co. RAG Assistant\")\n",
        "demo.launch(share=False)\n"
      ]
    }
  ]
}